{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPQMsVuMj4KdJXNJ/xL22SV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"KbN0ZJQg04Jj"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torchvision.transforms.functional as TF\n","\n","import os\n","from PIL import Image\n","from torch.utils.data import DataLoader\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","\n","from tqdm.auto import tqdm"]},{"cell_type":"code","source":["class UNET(nn.Module):\n","\n","  class DoubleConv(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","      super().__init__()\n","      self.conv = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size = (3,3), stride = 1, padding = 1, bias = False),\n","                                nn.BatchNorm2d(out_channels),\n","                                nn.ReLU(incplace = True),\n","                                nn.Conv2d(out_channels, out_channels, kernel_size = (3,3), stride = 1, padding = 1, bias = False),\n","                                nn.BatchNorm2d(out_channels),\n","                                nn.ReLU(incplace = True))\n","\n","    def forward(self, x):\n","      return self.conv(x)\n","\n","  def __init__(self, in_channels = 3, out_channels = 1, features =[64, 128, 256, 512]):\n","    super().__init__()\n","    self.downs = nn.ModuleList()\n","    self.ups = nn.ModuleList()\n","    self.pool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n","\n","    for feature in features:\n","      self.downs.append(DoubleConv(in_channels, feature))\n","      in_channels = feature\n","\n","    for feature in reversed(features):\n","      self.ups.append(nn.ConvTranspose2d(feature * 2, feature), kernel_size = 2, stride = 2)\n","      self.ups.append(DoubleConv(feature * 2, feature))\n","\n","    self.bottleneck = DoubleConv(features[-1], feature[-1] * 2)\n","    self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size = 1)\n","\n","  def forward(self, x):\n","    skip_connections = []\n","    for down in self.downs:\n","      x = down(x)\n","      skip_connections.append(x)\n","      x = self.pool(x)\n","\n","    x = self.bottleneck(x)\n","    for idx in range(0, len(self.ups), 2):\n","      x = self.ups[idx](x)\n","      skip_connection = skip_connections[idx // 2]\n","\n","      if x.shape != skip_connections.shape:\n","        x = TF.resize(x, size = skip_connections.shape[2:])\n","      concat_skip = torch.cat((skip_connection, x), dim = 1)\n","      x = self.ups[idx + 1](concat_skip)\n","\n","    x = self.final_conv(x)\n","\n","    return x\n","\n","\n","\n","\n","\n"],"metadata":{"id":"I1VQlYTh3PIJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CarvanaDataset(Dataset):\n","  def __init__(self, image_dir, mask_dir, transform = None, mode = 'train'):\n","    self.image_dir = image_dir\n","    self.mask_dir = mask_dir\n","    self.transform = transform\n","    self.images = os.listdir(image_dir)\n","\n","  def __len__(self):\n","    return len(self.images)\n","\n","  def __getitem__(self, index):\n","    img_path = os.path.join([self.image_dir, self.images[index]])\n","    mask_path = os.path.join([self.mask_dir, self.images[index].replace('.jpg', '_mask.gif')])\n","    image = np.array(Image.open(img_path).convert(\"RGB\"))\n","    mask = np.array(Image.open(mask_path).convert(\"L\"), dtype = np.float32)\n","    mask[mask == 255.0] = 1.0\n","\n","    if self.transform is not None:\n","      image, mask = self.transform(image = image, mask = mask)\n","\n","    return image, mask\n","\n"],"metadata":{"id":"WYUrNd3M3VZC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Hyperparameters e\n","LEARNING_RATE = 1e-4\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","BATCH_SIZE = 32\n","NUM_EPOCHS = 10\n","NUM_WORKERS = -1\n","IMAGE_HEIGHT = 160\n","IMAGE_WIDTH = 240\n","PIN_MEMORY = True\n","LOAD_MODEL = False\n","TRAIN_IMG_DIR = \"/content/train\"\n","TRAIN_MASK_DIR = \"/content/train_masks\""],"metadata":{"id":"7vsouQs3JCxB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip /content/train.zip\n","!unzip /content/train_masks.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SwNM_ZeGKD87","executionInfo":{"status":"ok","timestamp":1714736624101,"user_tz":-180,"elapsed":347,"user":{"displayName":"Bb Bb","userId":"08846771415044139021"}},"outputId":"97980763-ad2d-4360-c658-419b63771597"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  /content/train.zip\n","  End-of-central-directory signature not found.  Either this file is not\n","  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n","  latter case the central directory and zipfile comment will be found on\n","  the last disk(s) of this archive.\n","unzip:  cannot find zipfile directory in one of /content/train.zip or\n","        /content/train.zip.zip, and cannot find /content/train.zip.ZIP, period.\n"]}]},{"cell_type":"code","source":["os.listdir(TRAIN_IMG_DIR)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":141},"id":"9o1WyQqpJpB7","executionInfo":{"status":"error","timestamp":1714734830919,"user_tz":-180,"elapsed":256,"user":{"displayName":"Bb Bb","userId":"08846771415044139021"}},"outputId":"2d63b386-b8bf-4ba4-b618-67af28531366"},"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/train'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-fee61c8d37f1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_IMG_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/train'"]}]},{"cell_type":"code","source":["def train_fn(loader, model, optimizer, loss_fn, scaler):\n","  loop = tqdm(loader)\n","\n","  for bacth_idx, (data, target) in enumerate(loop):\n","    data = data.to(device = DEVICE)\n","    targets = targets.float().unsqueeze(1).to(device = DEVICE)\n","\n","    #forward\n","    with torch.cuda.amp.autocast():\n","      predicitions = model(data)\n","      loss = loss_fn(predicitions, targets)\n","\n","    #backward\n","    optimizer.zero_grad()\n","    scaler.scale(loss).backward()\n","    scaler.step(optimizer)\n","    scaler.update()\n","\n","    loop.set_postfix(loss = loss.item())\n"],"metadata":{"id":"uUVkllSOLfZy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_transform = transforms.Compose(\n","    [transforms.Resize(height = IMAGE_HEIGHT, width = IMAGE_WIDTH),\n","     transforms.Rotate(limit = 35, p = 1.0),\n","     transforms.HorizontalFlip(p = 0.5),\n","     transforms.VerticalFlip(p = 0.1),\n","     transforms.Normalize(mean = [0.0, 0.0, 0.0], std = [1.0, 1.0, 1.0],\n","                 max_pixel_value = 255.0),\n","     transforms.ToTensorV2(),],\n",")\n","\n","val_transform = transforms.Compose(\n","    [transforms.Resize(height = IMAGE_HEIGHT, width = IMAGE_WIDTH),\n","     transforms.Normalize(mean = [0.0, 0.0, 0.0], std = [1.0, 1.0, 1.0],\n","                 max_pixel_value = 255.0),\n","     transforms.ToTensorV2(),],\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"T5xzS42sMoCD","executionInfo":{"status":"error","timestamp":1714736028222,"user_tz":-180,"elapsed":396,"user":{"displayName":"Bb Bb","userId":"08846771415044139021"}},"outputId":"d2e39742-e12e-4259-96fd-827e4edfaa43"},"execution_count":null,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"Resize.__init__() got an unexpected keyword argument 'height'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-32-ad8e34aa142a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m train_transform = transforms.Compose(\n\u001b[0;32m----> 2\u001b[0;31m     [transforms.Resize(height = IMAGE_HEIGHT, width = IMAGE_WIDTH),\n\u001b[0m\u001b[1;32m      3\u001b[0m      \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m35\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m      \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHorizontalFlip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m      \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVerticalFlip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Resize.__init__() got an unexpected keyword argument 'height'"]}]},{"cell_type":"code","source":["model = UNET(in_channels = 3, out_channels = 1).to(DEVICE)\n","loss_fn = nn.BCEWithLogitsLoss()\n","optimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE)"],"metadata":{"id":"INbfs2YvNqHQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_loader, val_loader = get_loaders(TRAIN_IMG_DIR, TRAIN_MASK_DIR, train_transforms, val_transforms)\n"],"metadata":{"id":"nE_mt8JuOKQZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scaler = torch.cuda.amp.GradScaler()\n"],"metadata":{"id":"djuNbXGKO8Pl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for epoch in range(NUM_EPOCHS):\n","  train_fn = (train_loader, model, optimizer, loss_fn, scaler)\n","\n"],"metadata":{"id":"Vizu1XH8PCEn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def save_checkpoint(state, filename = \"checkpoint.pth.tar\"):\n","  print(\"=> Saving checkpoint\")\n","  torch.save(state, filename)\n","\n","def load_checkpoint(checkpoint, model):\n","  print(\"=> Loading checkpoint\")\n","  model.load_state_dict(checkpoint['state_dict'])\n","\n","def get_loaders(train_dir, train_maskdir, train_transform, val_transforms, num_workers = -1, pin_memory = True):\n","  train_dataset = CarvanaDataset(image_dir = train_dir, mask_dir = train_maskdir, transforms = train_transform)\n","  train_loader = DataLoader(train_dataset, batch_size = batch_size, num_workers= num_workers, pin_memory = pin_memory, shuffle = True)\n"],"metadata":{"id":"r7KdhQyiRLCk"},"execution_count":null,"outputs":[]}]}