{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNetDataset(Dataset):\n",
    "    def __init__(self,paths):\n",
    "        self.paths = paths\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        path = self.paths[idx]\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((128, 128)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        image = Image.open(path).convert('RGB')\n",
    "        image = transform(image)\n",
    "        \n",
    "        return image\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "\n",
    "train_folder_path = Path('C:\\\\Users\\\\user\\\\Training Models\\\\')\n",
    "train_files = list(train_folder_path.rglob('*.jpg'))\n",
    "\n",
    "test_dataset = ImageNetDataset(train_files)\n",
    "test_data_loader = DataLoader(test_dataset,batch_size=128,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAE(nn.Module):\n",
    "   \n",
    "    def __init__(self, num_embeddings = 512, embedding_dim = 64):\n",
    "        super().__init__()\n",
    "        self.act = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings = self.num_embeddings, embedding_dim = self.embedding_dim)\n",
    "        \n",
    "        self.enc_conv1 = nn.Conv2d(in_channels = 3, out_channels = 32, kernel_size = (4,4), stride = 2, padding = 1) #64x64\n",
    "        self.enc_conv2 = nn.Conv2d(in_channels = 32, out_channels = 128, kernel_size = (4,4), stride = 2, padding = 1) #32x32\n",
    "        \n",
    "         \n",
    "        self.enc_conv_res11 = nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = (3,3), stride = 1, padding = 1) #32x32\n",
    "        self.enc_conv_res12 = nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = (1,1), stride = 1, padding = 0) #32x32\n",
    "        \n",
    "        self.enc_conv_res21 = nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = (3,3), stride = 1, padding = 1) #32x32\n",
    "        self.enc_conv_res22 = nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = (1,1), stride = 1, padding = 0) #32x32\n",
    "        \n",
    "        self.pre_quantization_conv = nn.Conv2d(128, 64, kernel_size=1, stride=1, padding = 0)\n",
    "        \n",
    "        self.after_quantization_conv = nn.Conv2d(64, 128, kernel_size=1, stride=1, padding = 0)\n",
    "        \n",
    "        \n",
    "        self.dec_conv1 = torch.nn.ConvTranspose2d(in_channels = 32, out_channels = 3, kernel_size = (2,2), stride=2, padding=0)\n",
    "        self.dec_conv2 = torch.nn.ConvTranspose2d(in_channels = 128, out_channels = 32, kernel_size = (2,2), stride=2, padding=0)     \n",
    "        \n",
    "        self.dec_conv_res11 = nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = (3,3), stride = 1, padding = 1)\n",
    "        self.dec_conv_res12 = nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = (1,1), stride = 1, padding = 0)\n",
    "        \n",
    "        self.dec_conv_res21 = nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = (3,3), stride = 1, padding = 1)\n",
    "        self.dec_conv_res22 = nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = (1,1), stride = 1, padding = 0)\n",
    "        \n",
    "        # Commitment Loss Beta\n",
    "        self.beta = 0.25\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # B, C, H, W\n",
    "        x = self.act(self.enc_conv1(x))\n",
    "        x = self.act(self.enc_conv2(x))\n",
    "        x_clone = x.clone()\n",
    "        x = self.act(self.enc_conv_res11(x))\n",
    "        x = self.act(self.enc_conv_res12(x))\n",
    "        x = x_clone + x\n",
    "        x_clone = x.clone()\n",
    "        x = self.act(self.enc_conv_res21(x))\n",
    "        x = self.act(self.enc_conv_res22(x))\n",
    "        x = x_clone + x \n",
    "        \n",
    "        quant_input = self.pre_quantization_conv(x) #512 channels\n",
    "        \n",
    "        ## Quantization\n",
    "        B, C, H, W = quant_input.shape\n",
    "        quant_input = quant_input.permute(0, 2, 3, 1) # B, H, W, C\n",
    "        quant_input = quant_input.reshape((B, H * W, C))\n",
    "        \n",
    "        # Compute pairwise distances\n",
    "        dist = torch.cdist(quant_input, self.embedding.weight[None,:].repeat((B, 1, 1)) )\n",
    "        \n",
    "        # Find index of nearest embedding\n",
    "        min_encoding_indices = torch.argmin(dist, dim=-1)\n",
    "        \n",
    "        # Select the embedding weights\n",
    "        quant_out = torch.index_select(self.embedding.weight, 0, min_encoding_indices.view(-1))\n",
    "        quant_input = quant_input.reshape((-1, quant_input.size(-1)))\n",
    "        \n",
    "        commitment_loss = torch.mean((quant_input - quant_out.detach()) ** 2)\n",
    "        codebook_loss = torch.mean((quant_input.detach() - quant_out) ** 2)\n",
    "        quantize_losses = codebook_loss + self.beta*commitment_loss\n",
    "        \n",
    "        # Ensure straight through gradient\n",
    "        quant_out = quant_input + (quant_out - quant_input).detach() # Градиент будет течь через quant_input, так как будто бы quant_out и quant_input  - это одно и тоже, при этом их значения отличаются\n",
    "\n",
    "        quant_out = quant_out.reshape(B, H, W, C).permute(0, 3, 1, 2)\n",
    "        x = self.after_quantization_conv(quant_out)\n",
    "        \n",
    "        x_clone = x.clone()\n",
    "        x = self.act(self.dec_conv_res22(x))\n",
    "        x = self.act(self.dec_conv_res21(x))\n",
    "        x = x + x_clone\n",
    "        \n",
    "        \n",
    "        x_clone = x.clone()\n",
    "        x = self.act(self.dec_conv_res12(x))\n",
    "        x = self.act(self.dec_conv_res11(x))\n",
    "        x = x + x_clone\n",
    "        \n",
    "        \n",
    "        x = self.act(self.dec_conv2(x))\n",
    "        x = self.dec_conv1(x)\n",
    "        output = self.sigmoid(x)\n",
    "        \n",
    "        \n",
    "        return output, quantize_losses\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict = {}\n",
    "temp_dict = torch.load('C:\\\\Users\\\\user\\\\Training Models\\\\model_params.pt')[\"state_dict\"]\n",
    "\n",
    "for a, b in temp_dict.items():\n",
    "    new_dict[a[7:]] = b\n",
    "    \n",
    "#   new_dict['module.' + a] = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VQVAE(512, 64)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "#model = torch.nn.DataParallel(model, device_ids=(0,1,2,3))\n",
    "model.load_state_dict(torch.load('C:\\\\Users\\\\user\\\\Training Models\\\\new_model_params.pt')[\"state_dict\"])\n",
    "#model.load_state_dict(new_dict)\n",
    "loss_fn = nn.MSELoss(reduction=\"sum\")\n",
    "optim = torch.optim.Adam(model.parameters(), lr = 2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 70\n",
    "losses = []\n",
    "rec_losses = []\n",
    "for epoch in tqdm(range(1, num_epochs + 1)):\n",
    "    for index, data in enumerate(test_data_loader):\n",
    "\n",
    "        img = data.to(device)\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        res, loss_  = model(img)\n",
    "        loss__ = loss_fn(res, img)\n",
    "        loss_item = loss_ + loss__\n",
    "        model.zero_grad()\n",
    "        loss_item.sum().backward()\n",
    "        optim.step()\n",
    "        \n",
    "        if index != 0 and index % 100 == 0:\n",
    "            rec_losses.append(loss__.sum().item())\n",
    "            losses.append(loss_item.sum().item())\n",
    "            print(epoch, \"rec loss:\", round(rec_losses[-1],2), \"loss:\", round(losses[-1] / 100_000_000, 3) )\n",
    "        if index % 500 == 0:    \n",
    "            plt.figure(figsize=(20, 8))\n",
    "\n",
    "            for i in range(1, 6):\n",
    "                plt.subplot(2,5,i)\n",
    "                plt.imshow(img[i - 1].cpu().detach().permute(1,2,0).numpy())\n",
    "                plt.axis('off')\n",
    "            for i in range(6, 11):\n",
    "                plt.subplot(2,5, i)\n",
    "                plt.imshow(res[i - 6].cpu().detach().permute(1,2,0).numpy())\n",
    "                plt.axis('off')\n",
    "            plt.show()\n",
    "            \n",
    "        if index != 0 and index % 1000 == 0:\n",
    "            \n",
    "            state = {\n",
    "            'model': model,\n",
    "            'state_dict': model.state_dict()\n",
    "            }\n",
    "            torch.save(state, 'C:\\\\Users\\\\user\\\\Training Models\\\\new_model_params.pt')\n",
    "            print('model saved')\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
