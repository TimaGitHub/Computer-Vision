{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNetDataset(Dataset):\n",
    "    def __init__(self,paths):\n",
    "        self.paths = paths\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        path = self.paths[idx]\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        image = Image.open(path).convert('RGB')\n",
    "        image = transform(image)\n",
    "        \n",
    "        return image\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "\n",
    "train_folder_path = Path('C:\\\\Users\\\\user\\\\Training Models\\\\')\n",
    "train_files = list(train_folder_path.rglob('*.jpg'))\n",
    "\n",
    "test_dataset = ImageNetDataset(train_files)\n",
    "test_data_loader = DataLoader(test_dataset,batch_size=128,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualLayer(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = channels, out_channels = channels, kernel_size = (3,3), stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels = channels, out_channels = channels, kernel_size = (1,1), stride = 1, padding = 0),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.conv(x)\n",
    "    \n",
    "    \n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, num_blocks):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.res_blocks = nn.Sequential(\n",
    "            *[ResidualLayer(channels) for i in range(num_blocks)]\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.res_blocks(x)\n",
    "    \n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, stride_factor, num_blocks):\n",
    "        super().__init__()\n",
    "        blocks = []\n",
    "        \n",
    "        if stride_factor == 2:\n",
    "            blocks = [\n",
    "                nn.Conv2d(in_channel, out_channel // 2, (4,4), 2, 1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(out_channel // 2, out_channel, (4,4), 2, 1),\n",
    "                nn.ReLU()\n",
    "            ]\n",
    "            \n",
    "        elif stride_factor == 4:\n",
    "            blocks = [\n",
    "                nn.Conv2d(in_channel, out_channel // 2, (4,4), 2, 1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(out_channel // 2, out_channel, (4,4), 2, 1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(out_channel, out_channel, (4,4), 2, 1),\n",
    "                nn.ReLU(),\n",
    "            ]\n",
    "        blocks.append(ResidualBlock(out_channel, num_blocks))\n",
    "        \n",
    "        self.blocks = nn.Sequential(*blocks)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.blocks(x)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, stride_factor, num_blocks):\n",
    "        super().__init__()\n",
    "        blocks = []\n",
    "        \n",
    "        blocks.append(ResidualBlock(out_channel, num_blocks))\n",
    "        \n",
    "        if stride_factor == 2:\n",
    "            blocks += [\n",
    "                nn.ConvTranspose2d(out_channel, out_channel // 2, (2,2), 2, 0),\n",
    "                nn.ReLU(),\n",
    "                nn.ConvTranspose2d(out_channel // 2, in_channel, (2,2), 2, 0),\n",
    "                nn.ReLU()\n",
    "            ]\n",
    "            \n",
    "        elif stride_factor == 4:\n",
    "            blocks += [\n",
    "                nn.ConvTranspose2d(out_channel, out_channel, (2,2), 2, 0),\n",
    "                nn.ReLU(),\n",
    "            ]\n",
    "        \n",
    "        self.blocks = nn.Sequential(*blocks)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.blocks(x)\n",
    "    \n",
    "class Quantize(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, beta):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "        self.embedding = nn.Embedding(num_embeddings = num_embeddings, embedding_dim = embedding_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        quant_input = x\n",
    "        B, C, H, W = quant_input.shape\n",
    "        quant_input = quant_input.permute(0, 2, 3, 1) # B, H, W, C\n",
    "        quant_input = quant_input.reshape((B, H * W, C))\n",
    "        dist = torch.cdist(quant_input, self.embedding.weight[None,:].repeat((B, 1, 1)) )\n",
    "        min_encoding_indices = torch.argmin(dist, dim=-1)\n",
    "        quant_out = torch.index_select(self.embedding.weight, 0, min_encoding_indices.view(-1))\n",
    "        quant_input = quant_input.reshape((-1, quant_input.size(-1)))\n",
    "        \n",
    "        commitment_loss = torch.mean((quant_input - quant_out.detach()) ** 2)\n",
    "        codebook_loss = torch.mean((quant_input.detach() - quant_out) ** 2)\n",
    "        quantize_losses = codebook_loss + self.beta*commitment_loss\n",
    "        \n",
    "        quant_out = quant_input + (quant_out - quant_input).detach()\n",
    "\n",
    "        quant_out = quant_out.reshape(B, H, W, C).permute(0, 3, 1, 2)\n",
    "        \n",
    "        return quant_out, quantize_losses\n",
    "    \n",
    "\n",
    "class VQVAE2(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_embeddings = (512, 512), embedding_dim = (64, 64), in_channel = 3, out_channel = 128, num_blocks = 3):\n",
    "        super().__init__()\n",
    "        self.act = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.beta = 0.25\n",
    "        \n",
    "        self.enc_1 = Encoder(in_channel, out_channel,2, num_blocks)\n",
    "        self.enc_2 = Encoder(in_channel, out_channel,4, num_blocks)\n",
    "        \n",
    "        self.pre_quantization_conv = nn.Conv2d(out_channel, embedding_dim[0], kernel_size=1, stride=1, padding = 0)\n",
    "        self.pre_quantization_conv_cat = nn.Conv2d(out_channel*2, embedding_dim[0], kernel_size=1, stride=1, padding = 0)\n",
    "        \n",
    "        self.quant_1 = Quantize(num_embeddings[0], embedding_dim[0], self.beta)\n",
    "        self.quant_2 = Quantize(num_embeddings[1], embedding_dim[1], self.beta)\n",
    "        \n",
    "        self.after_quantization_conv = nn.Conv2d(embedding_dim[0], out_channel, kernel_size=1, stride=1, padding = 0)\n",
    "        \n",
    "        self.dec_1 = Decoder(in_channel, out_channel * 2, 2, num_blocks)\n",
    "        self.dec_2 = Decoder(out_channel, out_channel, 4, num_blocks)\n",
    "        \n",
    "        self.upsample_cat = nn.ConvTranspose2d(\n",
    "            out_channel, out_channel, 4, stride=2, padding=1\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # B, C, H, W\n",
    "        x_1 = self.enc_1(x)\n",
    "        x_2 = self.enc_2(x)\n",
    "        x_2 = self.pre_quantization_conv(x_2)\n",
    "        quant_input_2, quant_loss_2 = self.quant_2(x_2)\n",
    "        quant_input_2 = self.after_quantization_conv(quant_input_2)\n",
    "        quant_input_2_dec = self.dec_2(quant_input_2)\n",
    "        \n",
    "        x_1 = self.pre_quantization_conv_cat(torch.cat([x_1, quant_input_2_dec], dim = 1))\n",
    "        quant_input_1, quant_loss_1 = self.quant_1(x_1)\n",
    "        quant_input_1 = self.after_quantization_conv(quant_input_1)\n",
    "        \n",
    "        quant_input_1 = torch.cat([quant_input_1, self.upsample_cat(quant_input_2)], dim = 1)\n",
    "        \n",
    "        quant_input_1 = self.dec_1(quant_input_1)\n",
    "        \n",
    "        output = self.sigmoid(quant_input_1)\n",
    "        \n",
    "        return output, quant_loss_2, quant_loss_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict = {}\n",
    "temp_dict = torch.load('C:\\\\Users\\\\user\\\\Training Models\\\\model_params.pt')[\"state_dict\"]\n",
    "\n",
    "for a, b in temp_dict.items():\n",
    "    new_dict[a[7:]] = b\n",
    "    \n",
    "#   new_dict['module.' + a] = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VQVAE2(num_embeddings = (512, 512), embedding_dim = (64, 64), in_channel = 3, out_channel = 128, num_blocks = 3)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     model = torch.nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n",
    "    \n",
    "# load_dict = torch.load('/kaggle/working/model_params.pt')[\"state_dict\"]\n",
    "# new_load_dict = {}\n",
    "# for a, b in load_dict.items():\n",
    "#     if a[:6] == 'module':\n",
    "#         new_load_dict[a[6:]] = b\n",
    "#     else:\n",
    "#         new_load_dict = load_dict\n",
    "#         break\n",
    "        \n",
    "# model.load_state_dict(new_load_dict)\n",
    "loss_fn = nn.MSELoss(reduction=\"sum\")\n",
    "optim = torch.optim.Adam(model.parameters(), lr = 2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "losses = []\n",
    "rec_losses = []\n",
    "for epoch in tqdm(range(1, num_epochs + 1)):\n",
    "    for index, data in enumerate(test_data_loader):\n",
    "\n",
    "        img = data.to(device)\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        res, loss_enc2, loss_enc1  = model(img)\n",
    "        loss_dec = loss_fn(res, img)\n",
    "        loss_item = loss_enc2 + loss_enc1 + loss_dec\n",
    "        model.zero_grad()\n",
    "        loss_item.sum().backward()\n",
    "        optim.step()\n",
    "        \n",
    "        if index > 0 and index % 100 == 0:\n",
    "            rec_losses.append(loss_dec.sum().item())\n",
    "            losses.append(loss_item.sum().item())\n",
    "            print(epoch, \"rec loss:\", round(rec_losses[-1] / 1_000, 4), \"loss:\", round(losses[-1] / 1_000_000_000_0, 4) )\n",
    "        if index % 500 == 0:    \n",
    "            plt.figure(figsize=(20, 8))\n",
    "\n",
    "            for i in range(1, 6):\n",
    "                plt.subplot(2,5,i)\n",
    "                plt.imshow(img[i - 1].cpu().detach().permute(1,2,0).numpy())\n",
    "                plt.axis('off')\n",
    "            for i in range(6, 11):\n",
    "                plt.subplot(2,5, i)\n",
    "                plt.imshow(res[i - 6].cpu().detach().permute(1,2,0).numpy())\n",
    "                plt.axis('off')\n",
    "            plt.show()\n",
    "            \n",
    "        if index > 0 and index % 1000 == 0:\n",
    "            state = {\n",
    "            'model': model,\n",
    "            'state_dict': model.state_dict()\n",
    "            }\n",
    "            torch.save(state, 'C:\\\\Users\\\\user\\\\Training Models\\\\new_model_vqvae2_params.pt')\n",
    "            print('model saved')\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
